{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PsyTalk.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"7c_vfy-RvGhf","colab_type":"code","colab":{}},"source":["#By Salih Tutun-PsyTalk"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZKm-uJ7ovJ8t","colab_type":"text"},"source":["# **Config**\n"]},{"cell_type":"code","metadata":{"id":"67_AuQZTvMic","colab_type":"code","colab":{}},"source":["DATA_PATH = '/content/cornell movie-dialogs corpus'\n","CONVO_FILE = 'movie_conversations.txt'\n","LINE_FILE = 'movie_lines.txt'\n","OUTPUT_FILE = 'output_convo.txt'\n","PROCESSED_PATH = 'processed'\n","CPT_PATH = 'checkpoints'\n","\n","drive_folder= \"1Jz5_WZtPyH-7nQcWLnJBeRI6RP_1159i\"  ##### Remember this one since it's on my drive\n","# it will look like --> 1yABe4LsWeDQz5p5IO2AYJPosGOgqtD2Z\n","\n","\n","THRESHOLD = 2\n","\n","PAD_ID = 0\n","UNK_ID = 1\n","START_ID = 2\n","EOS_ID = 3\n","\n","TESTSET_SIZE = 25000\n","\n","#BUCKETS = [(19, 19), (28, 28), (33, 33), (40, 43), (50, 53), (60, 63)]\n","BUCKETS = [(8, 10), (12, 14), (16, 19)] ## use this one for now\n","#BUCKETS = [(6, 8), (8, 10), (10, 12), (13, 15), (16, 19), (19, 22), (23, 26), (29, 32), (39, 44)] \n","#BUCKETS = [(16, 19)]\n","\n","CONTRACTIONS = [(\"i ' m \", \"i 'm \"), (\"' d \", \"'d \"), (\"' s \", \"'s \"),\n","\t\t\t\t(\"don ' t \", \"do n't \"), (\"didn ' t \", \"did n't \"), (\"doesn ' t \", \"does n't \"),\n","\t\t\t\t(\"can ' t \", \"ca n't \"), (\"shouldn ' t \", \"should n't \"), (\"wouldn ' t \", \"would n't \"),\n","\t\t\t\t(\"' ve \", \"'ve \"), (\"' re \", \"'re \"), (\"in ' \", \"in' \")]\n","\n","NUM_LAYERS = 3\n","HIDDEN_SIZE = 256\n","BATCH_SIZE = 64\n","\n","LR = 0.5\n","MAX_GRAD_NORM = 5.0\n","\n","NUM_SAMPLES = 512 \n","ENC_VOCAB = 24397\n","DEC_VOCAB = 24649\n","ENC_VOCAB = 24459\n","DEC_VOCAB = 24608"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cJZ7VyqJvRhX","colab_type":"text"},"source":["# **Prepare Dataset**\n","Download and unzip Movie Dialogs Corpus"]},{"cell_type":"code","metadata":{"id":"tyVb8qxRvRBW","colab_type":"code","colab":{}},"source":["import urllib.request\n","urllib.request.urlretrieve('http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip', 'corpus.zip')\n","!unzip corpus.zip"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OmhG-BLovflr","colab_type":"code","colab":{}},"source":["!pip3 install config\n","!pip3 install tensorflow\n","!pip3 install pydrive\n","!pip3 install googledrivedownloader"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GvW-TwlKvgZV","colab_type":"code","colab":{}},"source":["import os\n","import random\n","import re\n","import numpy as np\n","#import config\n","\n","def get_lines():\n","    id2line = {}\n","    file_path = os.path.join(DATA_PATH, LINE_FILE)\n","    print(LINE_FILE)\n","    with open(file_path, 'r', errors='ignore') as f:\n","        # lines = f.readlines()\n","        # for line in lines:\n","        i = 0\n","        try:\n","            for line in f:\n","                parts = line.split(' +++$+++ ')\n","                if len(parts) == 5:\n","                    if parts[4][-1] == '\\n':\n","                        parts[4] = parts[4][:-1]\n","                    id2line[parts[0]] = parts[4]\n","                i += 1\n","        except UnicodeDecodeError:\n","            print(i, line)\n","    return id2line\n","\n","def get_convos():\n","    \"\"\" Get conversations from the raw data \"\"\"\n","    file_path = os.path.join(DATA_PATH, CONVO_FILE)\n","    convos = []\n","    with open(file_path, 'r') as f:\n","        for line in f.readlines():\n","            parts = line.split(' +++$+++ ')\n","            if len(parts) == 4:\n","                convo = []\n","                for line in parts[3][1:-2].split(', '):\n","                    convo.append(line[1:-1])\n","                convos.append(convo)\n","\n","    return convos\n","\n","def question_answers(id2line, convos):\n","    \"\"\" Divide the dataset into two sets: questions and answers. \"\"\"\n","    questions, answers = [], []\n","    for convo in convos:\n","        for index, line in enumerate(convo[:-1]):\n","            questions.append(id2line[convo[index]])\n","            answers.append(id2line[convo[index + 1]])\n","    assert len(questions) == len(answers)\n","    return questions, answers\n","\n","def prepare_dataset(questions, answers):\n","    # create path to store all the train & test encoder & decoder\n","    make_dir(PROCESSED_PATH)\n","\n","    # random convos to create the test set\n","    test_ids = random.sample([i for i in range(len(questions))],TESTSET_SIZE)\n","\n","    filenames = ['train.enc', 'train.dec', 'test.enc', 'test.dec']\n","    files = []\n","    for filename in filenames:\n","        files.append(open(os.path.join(PROCESSED_PATH, filename),'w'))\n","\n","    for i in range(len(questions)):\n","        if i in test_ids:\n","            files[2].write(questions[i] + '\\n')\n","            files[3].write(answers[i] + '\\n')\n","        else:\n","            files[0].write(questions[i] + '\\n')\n","            files[1].write(answers[i] + '\\n')\n","\n","    for file in files:\n","        file.close()\n","\n","def make_dir(path):\n","    \"\"\" Create a directory if there isn't one already. \"\"\"\n","    try:\n","        os.mkdir(path)\n","    except OSError:\n","        pass\n","\n","def basic_tokenizer(line, normalize_digits=True):\n","    \"\"\" A basic tokenizer to tokenize text into tokens.\n","    Feel free to change this to suit your need. \"\"\"\n","    line = re.sub('<u>', '', line)\n","    line = re.sub('</u>', '', line)\n","    line = re.sub('\\[', '', line)\n","    line = re.sub('\\]', '', line)\n","    words = []\n","    _WORD_SPLIT = re.compile(\"([.,!?\\\"'-<>:;)(])\")\n","    _DIGIT_RE = re.compile(r\"\\d\")\n","    for fragment in line.strip().lower().split():\n","        for token in re.split(_WORD_SPLIT, fragment):\n","            if not token:\n","                continue\n","            if normalize_digits:\n","                token = re.sub(_DIGIT_RE, '#', token)\n","            words.append(token)\n","    return words\n","\n","def build_vocab(filename, normalize_digits=True):\n","    in_path = os.path.join(PROCESSED_PATH, filename)\n","    out_path = os.path.join(PROCESSED_PATH, 'vocab.{}'.format(filename[-3:]))\n","\n","    vocab = {}\n","    with open(in_path, 'r') as f:\n","        for line in f.readlines():\n","            for token in basic_tokenizer(line):\n","                if not token in vocab:\n","                    vocab[token] = 0\n","                vocab[token] += 1\n","\n","    sorted_vocab = sorted(vocab, key=vocab.get, reverse=True)\n","    with open(out_path, 'w') as f:\n","        f.write('<pad>' + '\\n')\n","        f.write('<unk>' + '\\n')\n","        f.write('<s>' + '\\n')\n","        f.write('<\\s>' + '\\n')\n","        index = 4\n","        for word in sorted_vocab:\n","            if vocab[word] < THRESHOLD:\n","                break\n","            f.write(word + '\\n')\n","            index += 1\n","        with open('config.py', 'a') as cf:\n","            if filename[-3:] == 'enc':\n","                cf.write('ENC_VOCAB = ' + str(index) + '\\n')\n","            else:\n","                cf.write('DEC_VOCAB = ' + str(index) + '\\n')\n","\n","def load_vocab(vocab_path):\n","    with open(vocab_path, 'r') as f:\n","        words = f.read().splitlines()\n","    return words, {words[i]: i for i in range(len(words))}\n","\n","def sentence2id(vocab, line):\n","    return [vocab.get(token, vocab['<unk>']) for token in basic_tokenizer(line)]\n","\n","def token2id(data, mode):\n","    \"\"\" Convert all the tokens in the data into their corresponding\n","    index in the vocabulary. \"\"\"\n","    vocab_path = 'vocab.' + mode\n","    in_path = data + '.' + mode\n","    out_path = data + '_ids.' + mode\n","\n","    _, vocab = load_vocab(os.path.join(PROCESSED_PATH, vocab_path))\n","    in_file = open(os.path.join(PROCESSED_PATH, in_path), 'r')\n","    out_file = open(os.path.join(PROCESSED_PATH, out_path), 'w')\n","\n","    lines = in_file.read().splitlines()\n","    for line in lines:\n","        if mode == 'dec': # we only care about '<s>' and </s> in encoder\n","            ids = [vocab['<s>']]\n","        else:\n","            ids = []\n","        ids.extend(sentence2id(vocab, line))\n","        # ids.extend([vocab.get(token, vocab['<unk>']) for token in basic_tokenizer(line)])\n","        if mode == 'dec':\n","            ids.append(vocab['<\\s>'])\n","        out_file.write(' '.join(str(id_) for id_ in ids) + '\\n')\n","\n","def prepare_raw_data():\n","    print('Preparing raw data into train set and test set ...')\n","    id2line = get_lines()\n","    convos = get_convos()\n","    questions, answers = question_answers(id2line, convos)\n","    prepare_dataset(questions, answers)\n","\n","def process_data():\n","    print('Preparing data to be model-ready ...')\n","    build_vocab('train.enc')\n","    build_vocab('train.dec')\n","    token2id('train', 'enc')\n","    token2id('train', 'dec')\n","    token2id('test', 'enc')\n","    token2id('test', 'dec')\n","\n","def load_data(enc_filename, dec_filename, max_training_size=None):\n","    encode_file = open(os.path.join(PROCESSED_PATH, enc_filename), 'r')\n","    decode_file = open(os.path.join(PROCESSED_PATH, dec_filename), 'r')\n","    encode, decode = encode_file.readline(), decode_file.readline()\n","    data_buckets = [[] for _ in BUCKETS]\n","    i = 0\n","    while encode and decode:\n","        if (i + 1) % 10000 == 0:\n","            print(\"Bucketing conversation number\", i)\n","        encode_ids = [int(id_) for id_ in encode.split()]\n","        decode_ids = [int(id_) for id_ in decode.split()]\n","        for bucket_id, (encode_max_size, decode_max_size) in enumerate(BUCKETS):\n","            if len(encode_ids) <= encode_max_size and len(decode_ids) <= decode_max_size:\n","                data_buckets[bucket_id].append([encode_ids, decode_ids])\n","                break\n","        encode, decode = encode_file.readline(), decode_file.readline()\n","        i += 1\n","    return data_buckets\n","\n","def _pad_input(input_, size):\n","    return input_ + [PAD_ID] * (size - len(input_))\n","\n","def _reshape_batch(inputs, size, batch_size):\n","    \"\"\" Create batch-major inputs. Batch inputs are just re-indexed inputs\n","    \"\"\"\n","    batch_inputs = []\n","    for length_id in range(size):\n","        batch_inputs.append(np.array([inputs[batch_id][length_id]\n","                                    for batch_id in range(batch_size)], dtype=np.int32))\n","    return batch_inputs\n","\n","\n","def get_batch(data_bucket, bucket_id, batch_size=1):\n","    \"\"\" Return one batch to feed into the model \"\"\"\n","    # only pad to the max length of the bucket\n","    encoder_size, decoder_size = BUCKETS[bucket_id]\n","    encoder_inputs, decoder_inputs = [], []\n","\n","    for _ in range(batch_size):\n","        encoder_input, decoder_input = random.choice(data_bucket)\n","        # pad both encoder and decoder, reverse the encoder\n","        encoder_inputs.append(list(reversed(_pad_input(encoder_input, encoder_size))))\n","        decoder_inputs.append(_pad_input(decoder_input, decoder_size))\n","\n","    # now we create batch-major vectors from the data selected above.\n","    batch_encoder_inputs = _reshape_batch(encoder_inputs, encoder_size, batch_size)\n","    batch_decoder_inputs = _reshape_batch(decoder_inputs, decoder_size, batch_size)\n","\n","    # create decoder_masks to be 0 for decoders that are padding.\n","    batch_masks = []\n","    for length_id in range(decoder_size):\n","        batch_mask = np.ones(batch_size, dtype=np.float32)\n","        for batch_id in range(batch_size):\n","            # we set mask to 0 if the corresponding target is a PAD symbol.\n","            # the corresponding decoder is decoder_input shifted by 1 forward.\n","            if length_id < decoder_size - 1:\n","                target = decoder_inputs[batch_id][length_id + 1]\n","            if length_id == decoder_size - 1 or target == PAD_ID:\n","                batch_mask[batch_id] = 0.0\n","        batch_masks.append(batch_mask)\n","    return batch_encoder_inputs, batch_decoder_inputs, batch_masks\n","\n","if __name__ == '__main__':\n","    prepare_raw_data()\n","    process_data()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xLo1zcvXvr5I","colab_type":"text"},"source":["# **Prepare Chatbot**"]},{"cell_type":"code","metadata":{"id":"-qBZXWGAvm0c","colab_type":"code","colab":{}},"source":["import time\n","import numpy as np\n","import tensorflow as tf\n","\n","class ChatBotModel:\n","    def __init__(self, forward_only, batch_size):\n","        \"\"\"forward_only: if set, we do not construct the backward pass in the model.\n","        \"\"\"\n","        print('Initialize new model')\n","        self.fw_only = forward_only\n","        self.batch_size = batch_size\n","\n","    def _create_placeholders(self):\n","        # Feeds for inputs. It's a list of placeholders\n","        print('Create placeholders')\n","        self.encoder_inputs = [tf.placeholder(tf.int32, shape=[None], name='encoder{}'.format(i))\n","                               for i in range(BUCKETS[-1][0])]\n","        self.decoder_inputs = [tf.placeholder(tf.int32, shape=[None], name='decoder{}'.format(i))\n","                               for i in range(BUCKETS[-1][1] + 1)]\n","        self.decoder_masks = [tf.placeholder(tf.float32, shape=[None], name='mask{}'.format(i))\n","                              for i in range(BUCKETS[-1][1] + 1)]\n","\n","        # Our targets are decoder inputs shifted by one (to ignore <GO> symbol)\n","        self.targets = self.decoder_inputs[1:]\n","\n","    def _inference(self):\n","        print('Create inference')\n","        # If we use sampled softmax, we need an output projection.\n","        # Sampled softmax only makes sense if we sample less than vocabulary size.\n","        if NUM_SAMPLES > 0 and NUM_SAMPLES < DEC_VOCAB:\n","            w = tf.get_variable('proj_w', [HIDDEN_SIZE, DEC_VOCAB])\n","            b = tf.get_variable('proj_b', [DEC_VOCAB])\n","            self.output_projection = (w, b)\n","\n","        def sampled_loss(logits, labels):\n","            labels = tf.reshape(labels, [-1, 1])\n","            return tf.nn.sampled_softmax_loss(weights=tf.transpose(w),\n","                                              biases=b,\n","                                              inputs=logits,\n","                                              labels=labels,\n","                                              num_sampled=NUM_SAMPLES,\n","                                              num_classes=DEC_VOCAB)\n","        self.softmax_loss_function = sampled_loss\n","\n","        single_cell = tf.contrib.rnn.GRUCell(HIDDEN_SIZE)\n","        self.cell = tf.contrib.rnn.MultiRNNCell([single_cell for _ in range(NUM_LAYERS)])\n","\n","    def _create_loss(self):\n","        print('Creating loss... \\nIt might take a couple of minutes depending on how many buckets you have.')\n","        start = time.time()\n","        def _seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n","            setattr(tf.contrib.rnn.GRUCell, '__deepcopy__', lambda self, _: self)\n","            setattr(tf.contrib.rnn.MultiRNNCell, '__deepcopy__', lambda self, _: self)\n","            return tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n","                    encoder_inputs, decoder_inputs, self.cell,\n","                    num_encoder_symbols=ENC_VOCAB,\n","                    num_decoder_symbols=DEC_VOCAB,\n","                    embedding_size=HIDDEN_SIZE,\n","                    output_projection=self.output_projection,\n","                    feed_previous=do_decode)\n","\n","        if self.fw_only:\n","            self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(\n","                                        self.encoder_inputs,\n","                                        self.decoder_inputs,\n","                                        self.targets,\n","                                        self.decoder_masks,\n","                                        BUCKETS,\n","                                        lambda x, y: _seq2seq_f(x, y, True),\n","                                        softmax_loss_function=self.softmax_loss_function)\n","            # If we use output projection, we need to project outputs for decoding.\n","            if self.output_projection:\n","                for bucket in range(len(BUCKETS)):\n","                    self.outputs[bucket] = [tf.matmul(output,\n","                                            self.output_projection[0]) + self.output_projection[1]\n","                                            for output in self.outputs[bucket]]\n","        else:\n","            self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(\n","                                        self.encoder_inputs,\n","                                        self.decoder_inputs,\n","                                        self.targets,\n","                                        self.decoder_masks,\n","                                        BUCKETS,\n","                                        lambda x, y: _seq2seq_f(x, y, False),\n","                                        softmax_loss_function=self.softmax_loss_function)\n","        print('Time:', time.time() - start)\n","\n","    def _creat_optimizer(self):\n","        print('Create optimizer... \\nIt might take a couple of minutes depending on how many buckets you have.')\n","        with tf.variable_scope('training') as scope:\n","            self.global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n","\n","            if not self.fw_only:\n","                self.optimizer = tf.train.GradientDescentOptimizer(LR)\n","                trainables = tf.trainable_variables()\n","                self.gradient_norms = []\n","                self.train_ops = []\n","                start = time.time()\n","                for bucket in range(len(BUCKETS)):\n","\n","                    clipped_grads, norm = tf.clip_by_global_norm(tf.gradients(self.losses[bucket],\n","                                                                 trainables),\n","                                                                 MAX_GRAD_NORM)\n","                    self.gradient_norms.append(norm)\n","                    self.train_ops.append(self.optimizer.apply_gradients(zip(clipped_grads, trainables),\n","                                                            global_step=self.global_step))\n","                    print('Creating opt for bucket {} took {} seconds'.format(bucket, time.time() - start))\n","                    start = time.time()\n","\n","\n","    def _create_summary(self):\n","        pass\n","\n","    def build_graph(self):\n","        self._create_placeholders()\n","        self._inference()\n","        self._create_loss()\n","        self._creat_optimizer()\n","        self._create_summary()\n","print(\"all right!\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y4m2u8Bfvxny","colab_type":"code","colab":{}},"source":["import argparse\n","import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n","import random\n","import sys\n","import time\n","import numpy as np\n","import tensorflow as tf\n","\n","#from model import ChatBotModel\n","#import config\n","#import data\n","tf.reset_default_graph() \n","\n","def _get_random_bucket(train_buckets_scale):\n","    \"\"\" Get a random bucket from which to choose a training sample \"\"\"\n","    rand = random.random()\n","    return min([i for i in range(len(train_buckets_scale))\n","                if train_buckets_scale[i] > rand])\n","\n","def _assert_lengths(encoder_size, decoder_size, encoder_inputs, decoder_inputs, decoder_masks):\n","    \"\"\" Assert that the encoder inputs, decoder inputs, and decoder masks are\n","    of the expected lengths \"\"\"\n","    if len(encoder_inputs) != encoder_size:\n","        raise ValueError(\"Encoder length must be equal to the one in bucket,\"\n","                        \" %d != %d.\" % (len(encoder_inputs), encoder_size))\n","    if len(decoder_inputs) != decoder_size:\n","        raise ValueError(\"Decoder length must be equal to the one in bucket,\"\n","                       \" %d != %d.\" % (len(decoder_inputs), decoder_size))\n","    if len(decoder_masks) != decoder_size:\n","        raise ValueError(\"Weights length must be equal to the one in bucket,\"\n","                       \" %d != %d.\" % (len(decoder_masks), decoder_size))\n","\n","def run_step(sess, model, encoder_inputs, decoder_inputs, decoder_masks, bucket_id, forward_only):\n","    \"\"\" Run one step in training.\n","    @forward_only: boolean value to decide whether a backward path should be created\n","    forward_only is set to True when you just want to evaluate on the test set,\n","    or when you want to the bot to be in chat mode. \"\"\"\n","    encoder_size, decoder_size = BUCKETS[bucket_id]\n","    _assert_lengths(encoder_size, decoder_size, encoder_inputs, decoder_inputs, decoder_masks)\n","\n","    # input feed: encoder inputs, decoder inputs, target_weights, as provided.\n","    input_feed = {}\n","    for step in range(encoder_size):\n","        input_feed[model.encoder_inputs[step].name] = encoder_inputs[step]\n","    for step in range(decoder_size):\n","        input_feed[model.decoder_inputs[step].name] = decoder_inputs[step]\n","        input_feed[model.decoder_masks[step].name] = decoder_masks[step]\n","\n","    last_target = model.decoder_inputs[decoder_size].name\n","    input_feed[last_target] = np.zeros([model.batch_size], dtype=np.int32)\n","\n","    # output feed: depends on whether we do a backward step or not.\n","    if not forward_only:\n","        output_feed = [model.train_ops[bucket_id],  # update op that does SGD.\n","                       model.gradient_norms[bucket_id],  # gradient norm.\n","                       model.losses[bucket_id]]  # loss for this batch.\n","    else:\n","        output_feed = [model.losses[bucket_id]]  # loss for this batch.\n","        for step in range(decoder_size):  # output logits.\n","            output_feed.append(model.outputs[bucket_id][step])\n","\n","    outputs = sess.run(output_feed, input_feed)\n","    if not forward_only:\n","        return outputs[1], outputs[2], None  # Gradient norm, loss, no outputs.\n","    else:\n","        return None, outputs[0], outputs[1:]  # No gradient norm, loss, outputs.\n","\n","def _get_buckets():\n","    \"\"\" Load the dataset into buckets based on their lengths.\n","    train_buckets_scale is the inverval that'll help us\n","    choose a random bucket later on.\n","    \"\"\"\n","    test_buckets = load_data('test_ids.enc', 'test_ids.dec')\n","    data_buckets = load_data('train_ids.enc', 'train_ids.dec')\n","    train_bucket_sizes = [len(data_buckets[b]) for b in range(len(BUCKETS))]\n","    print(\"Number of samples in each bucket:\\n\", train_bucket_sizes)\n","    train_total_size = sum(train_bucket_sizes)\n","    # list of increasing numbers from 0 to 1 that we'll use to select a bucket.\n","    train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\n","                           for i in range(len(train_bucket_sizes))]\n","    print(\"Bucket scale:\\n\", train_buckets_scale)\n","    return test_buckets, data_buckets, train_buckets_scale\n","\n","def _get_skip_step(iteration):\n","    \"\"\" How many steps should the model train before it saves all the weights. \"\"\"\n","    if iteration < 100:\n","        return 30\n","    return 100\n","\n","\n","      \n","def _eval_test_set(sess, model, test_buckets):\n","    \"\"\" Evaluate on the test set. \"\"\"\n","    for bucket_id in range(len(BUCKETS)):\n","        if len(test_buckets[bucket_id]) == 0:\n","            print(\"  Test: empty bucket %d\" % (bucket_id))\n","            continue\n","        start = time.time()\n","        encoder_inputs, decoder_inputs, decoder_masks = get_batch(test_buckets[bucket_id],\n","                                                                        bucket_id,\n","                                                                        batch_size=BATCH_SIZE)\n","        _, step_loss, _ = run_step(sess, model, encoder_inputs, decoder_inputs,\n","                                   decoder_masks, bucket_id, True)\n","        print('Test bucket {}: loss {}, time {}'.format(bucket_id, step_loss, time.time() - start))\n","\n","def train():\n","    \"\"\" Train the bot \"\"\"\n","    test_buckets, data_buckets, train_buckets_scale = _get_buckets()\n","    # in train mode, we need to create the backward path, so forwrad_only is False\n","    model = ChatBotModel(False, BATCH_SIZE)\n","    model.build_graph()\n","\n","    saver = tf.train.Saver()\n","\n","    with tf.Session() as sess:\n","        print('Running session')\n","        sess.run(tf.global_variables_initializer())\n","        _check_restore_parameters(sess, saver)\n","\n","        iteration = model.global_step.eval()\n","        total_loss = 0\n","        while True:\n","            skip_step = _get_skip_step(iteration)\n","            bucket_id = _get_random_bucket(train_buckets_scale)\n","            encoder_inputs, decoder_inputs, decoder_masks = get_batch(data_buckets[bucket_id],\n","                                                                           bucket_id,\n","                                                                           batch_size=BATCH_SIZE)\n","            start = time.time()\n","            _, step_loss, _ = run_step(sess, model, encoder_inputs, decoder_inputs, decoder_masks, bucket_id, False)\n","            total_loss += step_loss\n","            iteration += 1\n","\n","            if iteration % skip_step == 0:\n","                print('Iter {}: loss {}, time {}'.format(iteration, total_loss/skip_step, time.time() - start))\n","                start = time.time()\n","                total_loss = 0\n","                saver.save(sess, os.path.join(CPT_PATH, 'chatbot'), global_step=model.global_step)\n","                \n","                \n","                \n","                \n","                if iteration % (10 * skip_step) == 0:\n","                    # Run evals on development set and print their loss\n","                    _eval_test_set(sess, model, test_buckets)\n","                    start = time.time()\n","                sys.stdout.flush()\n","\n","\n","def _find_right_bucket(length):\n","    \"\"\" Find the proper bucket for an encoder input based on its length \"\"\"\n","    return min([b for b in range(len(BUCKETS))\n","                if BUCKETS[b][0] >= length])\n","\n","def _construct_response(output_logits, inv_dec_vocab):\n","    \"\"\" Construct a response to the user's encoder input.\n","    @output_logits: the outputs from sequence to sequence wrapper.\n","    output_logits is decoder_size np array, each of dim 1 x DEC_VOCAB\n","\n","    This is a greedy decoder - outputs are just argmaxes of output_logits.\n","    \"\"\"\n","    print(output_logits[0])\n","    outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n","    # If there is an EOS symbol in outputs, cut them at that point.\n","    if EOS_ID in outputs:\n","        outputs = outputs[:outputs.index(EOS_ID)]\n","    # Print out sentence corresponding to outputs.\n","    return \" \".join([tf.compat.as_str(inv_dec_vocab[output]) for output in outputs])\n","def chat():\n","    \"\"\" in test mode, we don't to create the backward path\n","    \"\"\"\n","    _, enc_vocab = load_vocab(os.path.join(PROCESSED_PATH, 'vocab.enc'))\n","    inv_dec_vocab, _ = load_vocab(os.path.join(PROCESSED_PATH, 'vocab.dec'))\n","\n","    model = ChatBotModel(True, batch_size=1)\n","    model.build_graph()\n","\n","    saver = tf.train.Saver()\n","\n","    with tf.Session() as sess:\n","        sess.run(tf.global_variables_initializer())\n","        _check_restore_parameters(sess, saver)\n","        output_file = open(os.path.join(PROCESSED_PATH, OUTPUT_FILE), 'a+')\n","        # Decode from standard input.\n","        max_length = BUCKETS[-1][0]\n","        print('Welcome to TensorBro. Say something. Enter to exit. Max length is', max_length)\n","        while True:\n","            line = input(\"> \").strip(\"> \")\n","            #line = _get_user_input()\n","            if len(line) > 0 and line[-1] == '\\n':\n","                line = line[:-1]\n","            if line == '':\n","                break\n","            output_file.write('HUMAN ++++ ' + line + '\\n')\n","            # Get token-ids for the input sentence.\n","            token_ids = sentence2id(enc_vocab, str(line))\n","            if (len(token_ids) > max_length):\n","                print('Max length I can handle is:', max_length)\n","                line = input(\"> \").strip(\"> \")\n","                continue\n","                \n","            # Which bucket does it belong to?\n","            bucket_id = _find_right_bucket(len(token_ids))\n","            # Get a 1-element batch to feed the sentence to the model.\n","            encoder_inputs, decoder_inputs, decoder_masks = get_batch([(token_ids, [])],\n","                                                                            bucket_id,\n","                                                                            batch_size=1)\n","            # Get output logits for the sentence.\n","            _, _, output_logits = run_step(sess, model, encoder_inputs, decoder_inputs,\n","                                           decoder_masks, bucket_id, True)\n","            response = _construct_response(output_logits, inv_dec_vocab)\n","            print(response)\n","            output_file.write('BOT ++++ ' + response + '\\n')\n","        output_file.write('=============================================\\n')\n","        output_file.close()\n","        \n","print(\"all right!\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0syGHg_Sv4p0","colab_type":"text"},"source":["# **Train Chatbot**\n","The cool part arrives!\n","\n","if you want to upload your paramethers"]},{"cell_type":"code","metadata":{"id":"RedJOL7wv2GR","colab_type":"code","colab":{}},"source":["def _check_restore_parameters(sess, saver):       \n","    # Restore the previously trained parameters if there are any.\n","    ckpt = tf.train.get_checkpoint_state(os.path.dirname(CPT_PATH + '/checkpoint'))\n","    if ckpt and ckpt.model_checkpoint_path:\n","        print(\"Loading parameters for the Chatbot\")\n","        saver.restore(sess, ckpt.model_checkpoint_path)\n","    else:\n","      print(\"Initializing fresh parameters for the Chatbot\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V4eU0RANwAoO","colab_type":"code","colab":{}},"source":["tf.reset_default_graph() \n","train()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2K3oKPkBwFYu","colab_type":"text"},"source":["# **Test Chatbot**\n","And now let's test it"]},{"cell_type":"code","metadata":{"id":"e_NA-MNWwC3_","colab_type":"code","colab":{}},"source":["tf.reset_default_graph() \n","chat()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AN2HLRigwN-v","colab_type":"text"},"source":["# **Google Drive Connection**\n","login to your drive account"]},{"cell_type":"code","metadata":{"id":"LiB_ylSEwL3s","colab_type":"code","colab":{}},"source":["from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# 1. Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NDLpuIP5wZjx","colab_type":"text"},"source":["save file to Google Drive"]},{"cell_type":"code","metadata":{"id":"5aVpDdTVwWVh","colab_type":"code","colab":{}},"source":["import shutil\n","folder_id = drive_folder\n","\n","print(\"go go go!\")\n","zip_name = CPT_PATH\n","directory_name = '.'\n","directory_name = CPT_PATH\n","foo = shutil.make_archive(zip_name, 'zip', directory_name)\n","\n","# get the folder id where you want to save your file\n","file = drive.CreateFile({'parents':[{u'id': folder_id}]})\n","file.SetContentFile(foo)\n","file.Upload() \n","print(\"zip file saved on Drive\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0g7i-GNuwlZ0","colab_type":"text"},"source":["Import training data from my google drive\n","\n","add this to your drive: https://drive.google.com/file/d/1pXlLZ1fLgwl-W1gzAQjyIUe4Bh987uti/view\n","\n","Then make it a sharable link and copy the last part ex. 1pXlLZ1fLgwl-W1gzAQjyIUe4Bh98 and paste it in link_name"]},{"cell_type":"code","metadata":{"id":"t3b4uv5qwdp6","colab_type":"code","colab":{}},"source":["link_name = \"145SpA8qFPxwCCsDyVTydMln6s_CWN30E\" #insert yours"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A8QOatKHwtQs","colab_type":"code","colab":{}},"source":["from google_drive_downloader import GoogleDriveDownloader as gdd\n","\n","gdd.download_file_from_google_drive(file_id=link_name,\n","                                    dest_path='./checkpoints/upload.zip',\n","                                    unzip=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YbzvORhWww3o","colab_type":"code","colab":{}},"source":["import os\n","os.listdir('./')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MmpRXGehwxeT","colab_type":"code","colab":{}},"source":["os.listdir('./checkpoints')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cQ2Z6nEuw4O0","colab_type":"text"},"source":["\n","remove directory"]},{"cell_type":"code","metadata":{"id":"l7kPXTNQwzi4","colab_type":"code","colab":{}},"source":["import shutil\n","shutil.rmtree('./checkpoints')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X0aDXYPXwrAB","colab_type":"text"},"source":["create directory"]},{"cell_type":"code","metadata":{"id":"QUzSte3kxDwo","colab_type":"code","colab":{}},"source":["os.makedirs('./checkpoints')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iD9kvHEAxIlz","colab_type":"text"},"source":["download file on your computer"]},{"cell_type":"code","metadata":{"id":"IcLRa2mvxFxf","colab_type":"code","colab":{}},"source":["from google.colab import files\n","\n","files.download('./processed/output_convo.txt')  # from colab to browser download"],"execution_count":0,"outputs":[]}]}